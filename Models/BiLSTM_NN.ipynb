{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install matplotlib scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df1 = pd.read_json('domain1_train_data.json', lines=True)\n",
    "df2 = pd.read_json('domain2_train_data.json', lines=True)\n",
    "# Get domains\n",
    "df1['domain'],df2['domain'] = 0, 1\n",
    "\n",
    "# Train dev split\n",
    "df1_train, df1_dev = train_test_split(df1, stratify=df1['label'], random_state=42)\n",
    "df2_train, df2_dev = train_test_split(df2, stratify=df2['label'], random_state=42)\n",
    "\n",
    "# Join data in both domains for trating them jointly (Augmentation)\n",
    "df_train = pd.concat([df1_train, df2_train]).reset_index(drop=True)\n",
    "df_dev = pd.concat([df1_dev, df2_dev]).reset_index(drop=True)\n",
    "\n",
    "# Shuffle datasets\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_dev = df_dev.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 DL Models (BiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2189bda550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights function\n",
    "def weights(df):\n",
    "    w = list(len(df['label'])/df['label'].value_counts())\n",
    "    sample_weights = [0] * df.shape[0]\n",
    "    for idx, label in enumerate(df['label']):\n",
    "        sample_weights[idx] = w[label]\n",
    "    return sample_weights\n",
    "\n",
    "# Prepare pytorch dataset\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, text, labels, domain):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.domain = domain\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.tensor(self.text[idx])\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        domain = torch.tensor(self.domain[idx])\n",
    "        return text, label, domain\n",
    "    \n",
    "# Define collate (pre_process) function\n",
    "def collate_batch(batch):  \n",
    "    texts, labels, domain = zip(*batch)\n",
    "    text_len = [len(txt) for txt in texts]\n",
    "    text = nn.utils.rnn.pad_sequence(texts, batch_first=True).to(device)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32).to(device).reshape(-1,1)\n",
    "    domain = torch.tensor(domain, dtype=torch.float32).to(device).reshape(-1,1)\n",
    "    return text, labels, text_len, domain\n",
    "\n",
    "# Create datasets\n",
    "train_DS = Dataset(df_train['text'], df_train['label'], df_train['domain'])\n",
    "dev_DS = Dataset(df_dev['text'], df_dev['label'], df_train['domain'])\n",
    "\n",
    "# Sample train data\n",
    "sampler_tr = torch.utils.data.WeightedRandomSampler(weights(df_train), num_samples=len(train_DS), replacement=True)\n",
    "sampler_ts = torch.utils.data.WeightedRandomSampler(weights(df_dev), num_samples=len(dev_DS), replacement=True)\n",
    "\n",
    "# Create dataloaders\n",
    "bs = 32\n",
    "x_tr_dl = DataLoader(train_DS, batch_size=bs, collate_fn=collate_batch, sampler=sampler_tr)\n",
    "x_dev_dl = DataLoader(dev_DS, batch_size=bs, collate_fn=collate_batch, sampler=sampler_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/fungtion/DANN/blob/master/models/functions.py\n",
    "from torch.autograd import Function\n",
    "# Reverse layer for discriminator model\n",
    "class ReverseLayerF(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "        return output, None\n",
    "\n",
    "def reverse_gradient(x, alpha=3):\n",
    "    return ReverseLayerF.apply(x, alpha)\n",
    "\n",
    "# Bidirectional LSTM model\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embeding Layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        # BiLSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, num_layers=n_layers, batch_first=True, dropout = 0.5)\n",
    "        # Linear layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        # Sigmoid layer\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        # initial hidden and cell states\n",
    "        h0 = torch.zeros(self.n_layers * 2, text.size(0), self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.n_layers * 2, text.size(0), self.hidden_dim).to(device)\n",
    "        # Embeding \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # Batch packing\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
    "        # BiLSTM pass\n",
    "        output, (hidden, cell_state) = self.lstm(packed_embedded, (h0, c0))\n",
    "        # Concatenate hidden states in the BiLSTM\n",
    "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        # Dropout layer\n",
    "        hidden = self.dropout(hidden)\n",
    "        # Return the classifier's output\n",
    "        linear = self.fc(hidden)\n",
    "        # Sigmoid\n",
    "        return self.Sigmoid(linear.squeeze(0))\n",
    "\n",
    "# Discriminator model\n",
    "# Source: https://github.com/NaJaeMin92/pytorch-DANN/blob/master/model.py\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "                nn.Linear(hidden_dim*2, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        reversed_input = reverse_gradient(input_features)\n",
    "        print(reversed_input.shape)\n",
    "        x = self.discriminator(reversed_input)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM(\n",
      "  (embedding): Embedding(90000, 1024, padding_idx=0)\n",
      "  (lstm): LSTM(1024, 512, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (Sigmoid): Sigmoid()\n",
      ")\n",
      "Discriminator(\n",
      "  (discriminator): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = BiLSTM(vocab_size=90000, embedding_dim=128*8, hidden_dim=256*2, output_dim=1, n_layers=2).to(device)\n",
    "discriminator = Discriminator(hidden_dim=256*2).to(device)\n",
    "print(model)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss fn\n",
    "loss_fn = nn.BCELoss()\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(discriminator.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary accuracy function\n",
    "def binary_accuracy(predictions, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(predictions)).squeeze()  # Ensure it's a 1D tensor\n",
    "    correct = (rounded_preds == y).float()\n",
    "    return correct.sum() / len(correct)\n",
    "\n",
    "def compute_f1(predictions, labels):\n",
    "    # Convert predictions to binary\n",
    "    preds_binary = torch.round(torch.sigmoid(predictions))\n",
    "    preds_binary = preds_binary.detach().cpu().numpy()\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "\n",
    "    return f1_score(labels, preds_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "# Train\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    # Instanciate variables\n",
    "    train_loss, total = 0, 0\n",
    "    class_acc, c_real, c_preds = 0, [], []\n",
    "    domain_acc, d_preds, c_preds = 0, [], []\n",
    "\n",
    "    # Iterate dataloader\n",
    "    for X, y, text_len, domain in tqdm(dataloader):\n",
    "        # BilSTM\n",
    "        class_pred = model(X, text_len)   # Forward pass\n",
    "        class_loss = loss_fn(class_pred, y)     # Compute loss \n",
    "        class_acc += torch.sum((class_pred>=0.5).float() == y)\n",
    "        c_real.extend(y.int().detach().cpu().numpy().reshape(-1,1))\n",
    "        c_preds.extend((class_pred>=0.5).int().detach().cpu().numpy().reshape(-1,1))\n",
    "        \n",
    "        # Discriminator\n",
    "        domain_pred = discriminator(class_pred)\n",
    "        domain_loss = loss_fn(domain_pred, domain)\n",
    "        domain_acc += torch.sum((domain_pred>=0.5).float() == domain)\n",
    "        d_real.extend(domain.int().detach().cpu().numpy().reshape(-1,1))\n",
    "        d_preds.extend((domain_pred>=0.5).int().detach().cpu().numpy().reshape(-1,1))\n",
    "\n",
    "        # DANN loss\n",
    "        loss = class_loss + domain_loss\n",
    "        loss.backward()             # Backpropagation\n",
    "        optimizer.step()            # Update parameters\n",
    "        optimizer.zero_grad()       # Reset gradient\n",
    "    \n",
    "        # Metrics\n",
    "        train_loss += loss.item()\n",
    "        total += len(y)\n",
    "        \n",
    "    # General metrics\n",
    "    train_loss /= total\n",
    "    return train_loss, class_acc/size, f1_score(c_real, c_preds), domain_acc/size, f1_score(d_real, d_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "def test(dataloader, model, loss_fn, n_epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "\n",
    "    # Instanciate variables\n",
    "    test_loss, total = 0, 0\n",
    "    class_acc, c_real, c_preds = 0, [], []\n",
    "    domain_acc, d_preds, c_preds = 0, [], []\n",
    "\n",
    "    # Iterate dataloader\n",
    "    with torch.no_grad():       # Specify no gradient\n",
    "        for X, y, text_len, domain in tqdm(dataloader):\n",
    "            # BilSTM\n",
    "            class_pred = model(X, text_len)   # Forward pass\n",
    "            class_loss = loss_fn(class_pred, y)     # Compute loss \n",
    "            class_acc += torch.sum((class_pred>=0.5).float() == y)\n",
    "            c_real.extend(y.int().detach().cpu().numpy().reshape(-1,1))\n",
    "            c_preds.extend((class_pred>=0.5).int().detach().cpu().numpy().reshape(-1,1))\n",
    "            \n",
    "            # Discriminator\n",
    "            domain_pred = discriminator(class_pred)\n",
    "            domain_loss = loss_fn(domain_pred, domain)\n",
    "            domain_acc += torch.sum((domain_pred>=0.5).float() == domain)\n",
    "            d_real.extend(domain.int().detach().cpu().numpy().reshape(-1,1))\n",
    "            d_preds.extend((domain_pred>=0.5).int().detach().cpu().numpy().reshape(-1,1))\n",
    "\n",
    "            # DANN loss\n",
    "            loss = class_loss + domain_loss\n",
    "\n",
    "            # Metrics\n",
    "            test_loss += loss.item()                # Compute loss  \n",
    "            total += len(y)                         # Total observations\n",
    "\n",
    "    # General metrics\n",
    "    test_loss /= total\n",
    "    return test_loss, acc/size, f1_score(real, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BiLSTM network model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/422 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x1 and 1024x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining BiLSTM network model!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m----> 8\u001b[0m     tl, train_acc, f1_tr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tr_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     vl, valid_acc, f1_ts \u001b[38;5;241m=\u001b[39m test(x_dev_dl, model, loss_fn)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Losses to dict\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     20\u001b[0m c_preds\u001b[38;5;241m.\u001b[39mextend((class_pred\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Discriminator\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m domain_pred \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m domain_loss \u001b[38;5;241m=\u001b[39m loss_fn(domain_pred, domain)\n\u001b[1;32m     25\u001b[0m domain_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((domain_pred\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m==\u001b[39m domain)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 70\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, input_features)\u001b[0m\n\u001b[1;32m     68\u001b[0m reversed_input \u001b[38;5;241m=\u001b[39m reverse_gradient(input_features)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(reversed_input\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 70\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreversed_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x1 and 1024x512)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm, tqdm_notebook # show progress bar\n",
    "\n",
    "# Epochs\n",
    "epochs = 15\n",
    "train_loss, valid_loss = [], []\n",
    "print(\"Training BiLSTM network model!\")\n",
    "for t in range(epochs):\n",
    "    tl, train_acc, f1_tr = train(x_tr_dl, model, loss_fn, optimizer)\n",
    "    vl, valid_acc, f1_ts = test(x_dev_dl, model, loss_fn)\n",
    "    # Losses to dict\n",
    "    train_loss.append(tl)\n",
    "    valid_loss.append(vl)\n",
    "\n",
    "    # Print results\n",
    "    tqdm.write(\n",
    "        f'epoch #{t}\\ttrain_acc: {train_acc:.3f}\\tvalid_acc: {valid_acc:.3f}',\n",
    "    )\n",
    "    tqdm.write(\n",
    "        f'epoch #{t}\\tf1_tr: {f1_tr:.3f}\\tf1_ts: {f1_ts:.3f}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(10,5))\n",
    "epoch_ticks = range(1, epochs + 1)\n",
    "plt.plot(epoch_ticks, train_loss)\n",
    "plt.plot(epoch_ticks, valid_loss)\n",
    "plt.legend(['Train Loss', 'Valid Loss'])\n",
    "plt.title('Losses') \n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Loss')\n",
    "plt.xticks(epoch_ticks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dl, ln):\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        preds, acc, test_acc = [], 0, 0\n",
    "        for X, y, text_len in dl:\n",
    "            # Dev data\n",
    "            pred = model(X, text_len)\n",
    "            test_acc += torch.sum((pred>=0.5).float() == y)\n",
    "    return (test_acc/ln).detach().cpu().numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(x_tr_dl, df_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(x_dev_dl, df_dev.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('test_data.json', lines=True)['text']\n",
    "# train = [re.sub(',', '',', '.join([str(x) for x in tok])) for tok in df_train['text']]\n",
    "test = [[t if t != 0 else 1 for t in ls] for ls in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [i.split() for i in training_set]\n",
    "l_ = []\n",
    "for l in d:\n",
    "    l_.extend(l)\n",
    "corpus = Counter(l_)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for line in test:\n",
    "    text_tensor = torch.tensor(line).unsqueeze(0).to(device)\n",
    "    text_length = torch.tensor([len(line)])\n",
    "    # Pass the sequence and its length to the model\n",
    "    prediction = model(text_tensor, text_length)\n",
    "    preds.extend((prediction>=0.5).int().detach().cpu().numpy().reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(columns = ['id', 'value'])\n",
    "for idx, v in enumerate(preds):\n",
    "    test_df.loc[idx] = [idx, preds[idx]]\n",
    "test_df.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Export model\n",
    "model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "model_scripted.save('model_scripted.pt') # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = torch.jit.load('model_scripted.pt')\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
