{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install matplotlib scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    0.92\n",
      "1    0.08\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "df1 = pd.read_json('../Data/domain1_train_data.json', lines=True)\n",
    "df2 = pd.read_json('../Data/domain2_train_data.json', lines=True)\n",
    "\n",
    "# Define Domains\n",
    "df1['domain'], df2['domain'] = 0, 1\n",
    "\n",
    "# Split set 1\n",
    "df1_train, df1_dev = train_test_split(df1, stratify=df1['label'], random_state=0, test_size=0.2)\n",
    "# Split set 2\n",
    "x2_1 = df2[df2['label'] == 1].sample(500, random_state=0)\n",
    "x2_0 = df2[df2['label'] == 0].sample(500, random_state=0)\n",
    "df2_train = df2[[i not in list(pd.concat([x2_1, x2_0]).reset_index()['index']) for i in df2.index]].reset_index(drop=True)\n",
    "df2_dev = pd.concat([x2_1,x2_0]).reset_index(drop=True)\n",
    "\n",
    "# Print classes proportion\n",
    "print(round(df2_train['label'].value_counts()/len(df2_train['label']),2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 DL Models (BiLSTM + DANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc70674cf90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights function\n",
    "def weights(df):\n",
    "    w = list(len(df['label'])/df['label'].value_counts())\n",
    "    sample_weights = [0] * df.shape[0]\n",
    "    for idx, label in enumerate(df['label']):\n",
    "        sample_weights[idx] = w[label]\n",
    "    return sample_weights\n",
    "\n",
    "# Prepare pytorch dataset\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, text, labels, domain):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.domain = domain\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.tensor(self.text[idx])\n",
    "        label = torch.tensor(self.labels[idx]).reshape(-1,1)\n",
    "        domain = torch.tensor(self.domain[idx])\n",
    "        return text, label, domain\n",
    "    \n",
    "# Define collate (pre_process) function\n",
    "def collate_batch(batch):  \n",
    "    texts, labels, domain = zip(*batch)\n",
    "    text_len = [len(txt) for txt in texts]\n",
    "    text = nn.utils.rnn.pad_sequence(texts, batch_first=True).to(device)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32).to(device).reshape(-1,1)\n",
    "    domain = torch.tensor(domain, dtype=torch.float32).to(device).reshape(-1,1)\n",
    "    return text, labels, text_len, domain\n",
    "\n",
    "# Reset indexes\n",
    "df1_train.reset_index(drop=True, inplace=True)\n",
    "df2_train.reset_index(drop=True, inplace=True)\n",
    "df1_dev.reset_index(drop=True, inplace=True)\n",
    "df2_dev.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create datasets\n",
    "train_DS1 = Dataset(df1_train['text'], df1_train['label'], df1_train['domain'])\n",
    "train_DS2 = Dataset(df2_train['text'], df2_train['label'], df2_train['domain'])\n",
    "dev_DS1 = Dataset(df1_dev['text'], df1_dev['label'], df1_dev['domain'])\n",
    "dev_DS2 = Dataset(df2_dev['text'], df2_dev['label'], df2_dev['domain'])\n",
    "\n",
    "# Sample train data\n",
    "# sampler_tr1 = torch.utils.data.WeightedRandomSampler(weights(df1_train), num_samples=len(train_DS1), replacement=True)\n",
    "sampler_tr2 = torch.utils.data.WeightedRandomSampler(weights(df2_train), num_samples=len(train_DS2), replacement=True)\n",
    "\n",
    "# Create dataloaders\n",
    "bs = 32\n",
    "x_tr1 = DataLoader(train_DS1, batch_size=bs, collate_fn=collate_batch)\n",
    "x_tr2 = DataLoader(train_DS2, batch_size=bs, collate_fn=collate_batch, sampler=sampler_tr2)\n",
    "x_dev1 = DataLoader(dev_DS1, batch_size=bs, collate_fn=collate_batch)\n",
    "x_dev2 = DataLoader(dev_DS2, batch_size=bs, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "class ReverseLayerF(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "        return output, None\n",
    "\n",
    "def reverse_gradient(x, alpha=7):\n",
    "    return ReverseLayerF.apply(x, alpha)\n",
    "\n",
    "# Bidirectional LSTM model\n",
    "class DANN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers):\n",
    "        super(DANN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Feature extraction Layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, num_layers=n_layers, batch_first=True,dropout = 0.5)\n",
    "\n",
    "        # Classiffier layer\n",
    "        self.class_classifier = nn.Sequential()\n",
    "        self.class_classifier.add_module('fc1', nn.Linear(hidden_dim*2, 1))\n",
    "        self.class_classifier.add_module('dropout', nn.Dropout(0.2))\n",
    "        self.class_classifier.add_module('Sigmoid', nn.Sigmoid())\n",
    "\n",
    "        # Domain classifier Layer\n",
    "        self.domain_classifier = nn.Sequential()\n",
    "        self.domain_classifier.add_module('fc1', nn.Linear(hidden_dim*2, hidden_dim))\n",
    "        self.domain_classifier.add_module('relu', nn.ReLU())\n",
    "        self.domain_classifier.add_module('fc2', nn.Linear(hidden_dim, 1))\n",
    "        self.domain_classifier.add_module('sigmoid', nn.Sigmoid())\n",
    "\n",
    "    def forward(self, text, text_lengths, alpha):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
    "        output, (hidden, cell_state) = self.lstm(packed_embedded)\n",
    "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        reverse_feature = ReverseLayerF.apply(hidden, alpha)\n",
    "        class_output = self.class_classifier(hidden)\n",
    "        domain_output = self.domain_classifier(reverse_feature)\n",
    "        \n",
    "        return class_output, domain_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANN(\n",
      "  (embedding): Embedding(90000, 128, padding_idx=0)\n",
      "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (class_classifier): Sequential(\n",
      "    (fc1): Linear(in_features=512, out_features=1, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (Sigmoid): Sigmoid()\n",
      "  )\n",
      "  (domain_classifier): Sequential(\n",
      "    (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "h_dim, e_dim = 256, 128\n",
    "model = DANN(vocab_size=90000, embedding_dim=e_dim, hidden_dim=h_dim, n_layers=2).to(device)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.005, patience=5,)\n",
    "\n",
    "# Weights function\n",
    "def weights_class(y, c):\n",
    "    y = pd.Series(y.int().numpy(force=True).reshape(-1))\n",
    "    w = len(y)/y.value_counts()\n",
    "    if c == 0:\n",
    "        try:\n",
    "            return w[0]\n",
    "        except:\n",
    "            return 1\n",
    "    else:\n",
    "        try:\n",
    "            return w[1]\n",
    "        except:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ____________________________________________________________________________________________________________\n",
    "# Function to get models' metrics\n",
    "def model_metrics(dataloader_iter, alpha):\n",
    "    # training model using source data\n",
    "    X, y, text_len, domain = next(dataloader_iter)\n",
    "    # Predict\n",
    "    class_output, domain_output = model(X, text_len, alpha)\n",
    "    # Loss fn\n",
    "    loss_fn_cl = nn.BCELoss(weight = torch.tensor(weights_class(y, 0)).to(device))\n",
    "    loss_fn_d = nn.BCELoss(weight = torch.tensor(weights_class(domain, 1)).to(device))\n",
    "    # Classifier metrics\n",
    "    class_loss = loss_fn_cl(class_output, y) \n",
    "    class_acc = torch.sum((class_output>=0.5).float() == y)\n",
    "    # Discriminator metrics\n",
    "    domain_loss = loss_fn_d(domain_output, domain)\n",
    "    domain_acc = torch.sum((domain_output>=0.5).float() == domain)\n",
    "    total = y.size()[0]\n",
    "    return class_acc, domain_acc, total, class_loss, domain_loss\n",
    "# ____________________________________________________________________________________________________________\n",
    "# ____________________________________________________________________________________________________________\n",
    "# Helper function to return training metrics\n",
    "def train_model():\n",
    "    # Instanciate metric's variables\n",
    "    train_loss, total = 0, 0\n",
    "    class_acc1, class_acc2, tot1, tot2 = 0, 0, 0, 0\n",
    "    domain_acc1, domain_acc2= 0, 0\n",
    "    # Train parameters\n",
    "    len_dataloader = min(len(x_tr1), len(x_tr2))\n",
    "    data_source_iter = iter(x_tr1)\n",
    "    data_target_iter = iter(x_tr2)\n",
    "    # Iterate dataloader\n",
    "    for i in tqdm(range(len_dataloader)):\n",
    "        model.train()\n",
    "        # Calculate Alpha\n",
    "        p = float(i + epoch * len_dataloader) / epochs / len_dataloader\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "        \n",
    "        # Reset gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Run model\n",
    "        cl_a1, d_a1, t1, cl1, dl1 = model_metrics(data_source_iter, alpha)\n",
    "        cl_a2, d_a2, t2, cl2, dl2 = model_metrics(data_target_iter, alpha)\n",
    "        # Metrics\n",
    "        class_acc1 += cl_a1.cpu().numpy()\n",
    "        class_acc2 += cl_a2.cpu().numpy()\n",
    "        domain_acc1 += d_a1.cpu().numpy()\n",
    "        domain_acc2 += d_a2.cpu().numpy()\n",
    "        tot1 += t1\n",
    "        tot2 += t2\n",
    "        loss = cl1 + cl2\n",
    "        # Metrics\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()             # Backpropagation\n",
    "        optimizer.step()            # Update parameters\n",
    "        # lr_sched.step(train_loss)\n",
    "    # Print results\n",
    "    d_acc = (domain_acc1 + domain_acc2)/(tot1 + tot2)\n",
    "    cl1_acc = class_acc1/ tot1\n",
    "    cl2_acc = class_acc2/tot2\n",
    "    loss = train_loss/len_dataloader\n",
    "    \n",
    "    tqdm.write(\n",
    "        f'Domain_Acc: {d_acc:.3f}\\\n",
    "        Class1_Acc: {cl1_acc:.3f}\\\n",
    "        Class2_Acc: {cl2_acc:.3f}\\\n",
    "        Loss: {loss:.3f}',\n",
    "    )\n",
    "    # ____________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ____________________________________________________________________________________________________________\n",
    "# ____________________________________________________________________________________________________________\n",
    "# Helper function to return test metrics\n",
    "def test_model():\n",
    "    # Instanciate metric's variables\n",
    "    test_loss, total = 0, 0\n",
    "    class_acc1, class_acc2, tot1, tot2 = 0, 0, 0, 0\n",
    "    domain_acc1, domain_acc2= 0, 0\n",
    "\n",
    "    # Test parameters\n",
    "    len_dataloader = min(len(x_dev1), len(x_dev2))\n",
    "    data_source_iter = iter(x_dev1)\n",
    "    data_target_iter = iter(x_dev2)\n",
    "    \n",
    "    # Iterate dataloader\n",
    "    for i in tqdm(range(len_dataloader)):\n",
    "        model.eval()\n",
    "        # Calculate Alpha\n",
    "        p = float(i + epoch * len_dataloader) / epochs / len_dataloader\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "        \n",
    "        # Reset gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Run model\n",
    "        cl_a1, d_a1, t1, cl1, dl1 = model_metrics(data_source_iter, alpha)\n",
    "        cl_a2, d_a2, t2, cl2, dl2 = model_metrics(data_target_iter, alpha)\n",
    "\n",
    "        # Metrics\n",
    "        class_acc1 += cl_a1.cpu().numpy()\n",
    "        class_acc2 += cl_a2.cpu().numpy()\n",
    "        domain_acc1 += d_a1.cpu().numpy()\n",
    "        domain_acc2 += d_a2.cpu().numpy()\n",
    "        tot1 += t1\n",
    "        tot2 += t2\n",
    "        loss = cl1 + dl2\n",
    "    \n",
    "        # Metrics\n",
    "        test_loss += loss.item()\n",
    "\n",
    "    # Print results\n",
    "    d_acc = (domain_acc1 + domain_acc2)/(tot1 + tot2)\n",
    "    cl1_acc = class_acc1/ tot1\n",
    "    cl2_acc = class_acc2/tot2\n",
    "    loss = test_loss/len_dataloader\n",
    "    \n",
    "    tqdm.write(\n",
    "        f'Domain_Acc: {d_acc:.3f}\\\n",
    "        Class1_Acc: {cl1_acc:.3f}\\\n",
    "        Class2_Acc: {cl2_acc:.3f}\\\n",
    "        Loss: {loss:.3f}',\n",
    "    )\n",
    "    # ____________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BiLSTM network model!\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:26<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.539        Class1_Acc: 0.862        Class2_Acc: 0.863        Loss: 1.048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 11.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.585        Class1_Acc: 0.761        Class2_Acc: 0.688        Loss: 2.029\n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:26<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.527        Class1_Acc: 0.878        Class2_Acc: 0.880        Loss: 0.831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.579        Class1_Acc: 0.770        Class2_Acc: 0.689        Loss: 2.143\n",
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:26<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.536        Class1_Acc: 0.886        Class2_Acc: 0.884        Loss: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 11.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.622        Class1_Acc: 0.766        Class2_Acc: 0.678        Loss: 2.304\n",
      "epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:26<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.541        Class1_Acc: 0.889        Class2_Acc: 0.882        Loss: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.589        Class1_Acc: 0.740        Class2_Acc: 0.679        Loss: 2.438\n",
      "epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:26<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.546        Class1_Acc: 0.893        Class2_Acc: 0.888        Loss: 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 11.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.605        Class1_Acc: 0.771        Class2_Acc: 0.688        Loss: 2.511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm, tqdm_notebook # show progress bar\n",
    "\n",
    "# Epochs\n",
    "epochs = 5\n",
    "train_loss, valid_loss1, valid_loss2 = [], [], []\n",
    "print(\"Training BiLSTM network model!\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: %d'% (epoch))\n",
    "    train_model()\n",
    "    test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dl, ln):\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        preds, acc, test_acc = [], 0, 0\n",
    "        for X, y, text_len, domain in dl:\n",
    "            # Dev data\n",
    "            pred, domain_output = model(X, text_len, 1)\n",
    "            preds.extend((pred>=0.5).int().detach().cpu().numpy().reshape(-1))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    0.81\n",
      "1    0.19\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Join data in both domains for trating them jointly (Augmentation)\n",
    "dev_set = pd.concat([df1_dev, df2_dev]).reset_index(drop=True)\n",
    "train_set = pd.concat([df1_train, df2_train]).reset_index(drop=True)\n",
    "\n",
    "# Train and dev sets\n",
    "x_tr, y_train = train_set['text'], train_set['label']\n",
    "x_dev, y_dev = dev_set['text'], dev_set['label']\n",
    "\n",
    "# Print classes proportion\n",
    "print(round(train_set['label'].value_counts()/len(train_set['label']),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_DS = Dataset(train_set['text'], train_set['label'], train_set['domain'])\n",
    "dev_DS = Dataset(dev_set['text'], dev_set['label'], dev_set['domain'])\n",
    "\n",
    "# Sample train data\n",
    "sampler_tr = torch.utils.data.WeightedRandomSampler(weights(train_set), num_samples=len(train_DS), replacement=True)\n",
    "\n",
    "# Create dataloaders\n",
    "bs = 32\n",
    "x_tr_ = DataLoader(train_DS, batch_size=bs, collate_fn=collate_batch, sampler=sampler_tr)\n",
    "x_ts_ = DataLoader(dev_DS, batch_size=bs, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.497125"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_pr = predict(x_tr_, train_set.shape[0])\n",
    "sum(tr_pr == y_train)/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pr = predict(x_ts_, dev_set.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.730        F1 Score: 0.674        ROC_AUC: 0.730\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(f'Accuracy: {sum(dev_pr == y_dev)/len(y_dev):.3f}\\\n",
    "        F1 Score: {f1_score(y_dev, dev_pr):.3f}\\\n",
    "        ROC_AUC: {roc_auc_score(y_dev, dev_pr):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('../Data/test_data.json', lines=True)['text']\n",
    "test = [[t if t != 0 else 1 for t in ls] for ls in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DS = Dataset(test, [1]*len(test), [1]*len(test))\n",
    "x_tst = DataLoader(test_DS, batch_size=bs, collate_fn=collate_batch)\n",
    "preds, acc, test_acc = [], 0, 0\n",
    "for X, y, text_len, domain in x_tst:\n",
    "    # Dev data\n",
    "    pred, domain_output = model(X, text_len, 1)\n",
    "    preds.extend((pred>=0.5).int().detach().cpu().numpy().reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(columns = ['id', 'value'])\n",
    "for idx, v in enumerate(preds):\n",
    "    test_df.loc[idx] = [idx, preds[idx]]\n",
    "test_df.to_csv('../Data/predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value\n",
       "0    2605\n",
       "1    1395\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['value'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
