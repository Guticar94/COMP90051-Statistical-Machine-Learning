{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install matplotlib scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df1 = pd.read_json('../Data/domain1_train_data.json', lines=True)\n",
    "df2 = pd.read_json('../Data/domain2_train_data.json', lines=True)\n",
    "# df2, _ = RandomOverSampler(random_state=42).fit_resample(df2, df2['label'])\n",
    "# df2 = pd.concat([df2[df2['label'] == 0].sample(1500), df2[df2['label'] == 1]]).sample(frac=1).reset_index(drop=True)\n",
    "# Get domains\n",
    "df1['domain'],df2['domain'] = 0, 1\n",
    "\n",
    "# # Train dev split\n",
    "df1_train, df1_dev = train_test_split(df1, random_state=42)\n",
    "df2_train, df2_dev = train_test_split(df2, random_state=42)\n",
    "\n",
    "# # Join data in both domains for trating them jointly (Augmentation)\n",
    "# df_train = pd.concat([df1_train, df2_train]).reset_index(drop=True)\n",
    "\n",
    "# # Reset index\n",
    "# df1_dev = df1_dev.reset_index(drop=True)\n",
    "# df2_dev = df2_dev.reset_index(drop=True)\n",
    "\n",
    "# # Shuffle datasets\n",
    "# df_train = df_train.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 DL Models (BiLSTM + DANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff04bde5d30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare pytorch dataset\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, text, labels, domain):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.domain = domain\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.tensor(self.text[idx])\n",
    "        label = torch.tensor(self.labels[idx]).reshape(-1,1)\n",
    "        domain = torch.tensor(self.domain[idx])\n",
    "        return text, label, domain\n",
    "    \n",
    "# Define collate (pre_process) function\n",
    "def collate_batch(batch):  \n",
    "    texts, labels, domain = zip(*batch)\n",
    "    text_len = [len(txt) for txt in texts]\n",
    "    text = nn.utils.rnn.pad_sequence(texts, batch_first=True).to(device)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32).to(device).reshape(-1,1)\n",
    "    domain = torch.tensor(domain, dtype=torch.float32).to(device).reshape(-1,1)\n",
    "    return text, labels, text_len, domain\n",
    "\n",
    "# Rest indexes \n",
    "df1_train.reset_index(drop=True, inplace=True)\n",
    "df1_dev.reset_index(drop=True, inplace=True)\n",
    "df2_train.reset_index(drop=True, inplace=True)\n",
    "df2_dev.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create datasets\n",
    "train_DS1 = Dataset(df1_train['text'], df1_train['label'], df1_train['domain'])\n",
    "train_DS2 = Dataset(df2_train['text'], df2_train['label'], df2_train['domain'])\n",
    "dev_DS1 = Dataset(df1_dev['text'], df1_dev['label'], df1_dev['domain'])\n",
    "dev_DS2 = Dataset(df2_dev['text'], df2_dev['label'], df2_dev['domain'])\n",
    "\n",
    "# Create dataloaders\n",
    "bs = 32\n",
    "x_tr1 = DataLoader(train_DS1, batch_size=bs, collate_fn=collate_batch)\n",
    "x_tr2 = DataLoader(train_DS2, batch_size=bs, collate_fn=collate_batch)\n",
    "x_dev1 = DataLoader(dev_DS1, batch_size=bs, collate_fn=collate_batch)\n",
    "x_dev2 = DataLoader(dev_DS2, batch_size=bs, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "class ReverseLayerF(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "        return output, None\n",
    "\n",
    "def reverse_gradient(x, alpha=7):\n",
    "    return ReverseLayerF.apply(x, alpha)\n",
    "\n",
    "# Bidirectional LSTM model\n",
    "class DANN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers):\n",
    "        super(DANN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Feature extraction Layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, num_layers=n_layers, batch_first=True,dropout = 0.5)\n",
    "\n",
    "        # Classiffier layer\n",
    "        self.class_classifier = nn.Sequential()\n",
    "        self.class_classifier.add_module('fc1', nn.Linear(hidden_dim*2, 1))\n",
    "        self.class_classifier.add_module('dropout', nn.Dropout(0.2))\n",
    "        self.class_classifier.add_module('Sigmoid', nn.Sigmoid())\n",
    "\n",
    "        # Domain classifier Layer\n",
    "        self.domain_classifier = nn.Sequential()\n",
    "        self.domain_classifier.add_module('fc1', nn.Linear(hidden_dim*2, hidden_dim))\n",
    "        self.domain_classifier.add_module('relu', nn.ReLU())\n",
    "        self.domain_classifier.add_module('fc2', nn.Linear(hidden_dim, 1))\n",
    "        self.domain_classifier.add_module('sigmoid', nn.Sigmoid())\n",
    "\n",
    "    def forward(self, text, text_lengths, alpha):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
    "        output, (hidden, cell_state) = self.lstm(packed_embedded)\n",
    "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        reverse_feature = ReverseLayerF.apply(hidden, alpha)\n",
    "        class_output = self.class_classifier(hidden)\n",
    "        domain_output = self.domain_classifier(reverse_feature)\n",
    "        \n",
    "        return class_output, domain_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANN(\n",
      "  (embedding): Embedding(90000, 256, padding_idx=0)\n",
      "  (lstm): LSTM(256, 1024, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (class_classifier): Sequential(\n",
      "    (fc1): Linear(in_features=2048, out_features=1, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (Sigmoid): Sigmoid()\n",
      "  )\n",
      "  (domain_classifier): Sequential(\n",
      "    (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (fc2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "h_dim, e_dim = 256*4, 128*2\n",
    "model = DANN(vocab_size=90000, embedding_dim=e_dim, hidden_dim=h_dim, n_layers=2).to(device)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.005, patience=5,)\n",
    "\n",
    "# Weights function\n",
    "def weights_class(y, c):\n",
    "    y = pd.Series(y.int().numpy(force=True).reshape(-1))\n",
    "    w = len(y)/y.value_counts()\n",
    "    if c == 0:\n",
    "        try:\n",
    "            return w[0]\n",
    "        except:\n",
    "            return 1\n",
    "    else:\n",
    "        try:\n",
    "            return w[1]\n",
    "        except:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ____________________________________________________________________________________________________________\n",
    "# Function to get models' metrics\n",
    "def model_metrics(dataloader_iter, alpha):\n",
    "    # training model using source data\n",
    "    X, y, text_len, domain = next(dataloader_iter)\n",
    "    \n",
    "    # Predict\n",
    "    class_output, domain_output = model(X, text_len, alpha)\n",
    "    \n",
    "    # Loss fn\n",
    "    loss_fn_cl = nn.BCELoss(weight = torch.tensor(weights_class(y, 0)).to(device))\n",
    "    loss_fn_d = nn.BCELoss(weight = torch.tensor(weights_class(domain, 1)).to(device))\n",
    "\n",
    "    # Classifier metrics\n",
    "    class_loss = loss_fn_cl(class_output, y) \n",
    "    class_acc = torch.sum((class_output>=0.5).float() == y)\n",
    "    \n",
    "    # Discriminator metrics\n",
    "    domain_loss = loss_fn_d(domain_output, domain)\n",
    "    domain_acc = torch.sum((domain_output>=0.5).float() == domain)\n",
    "    total = y.size()[0]\n",
    "    \n",
    "    return class_acc, domain_acc, total, class_loss, domain_loss\n",
    "# ____________________________________________________________________________________________________________\n",
    "# ____________________________________________________________________________________________________________\n",
    "# Helper function to return training metrics\n",
    "def train_model():\n",
    "    # Instanciate metric's variables\n",
    "    train_loss, total = 0, 0\n",
    "    class_acc1, class_acc2, tot1, tot2 = 0, 0, 0, 0\n",
    "    domain_acc1, domain_acc2= 0, 0\n",
    "\n",
    "    # Train parameters\n",
    "    len_dataloader = min(len(x_tr1), len(x_tr2))\n",
    "    data_source_iter = iter(x_tr1)\n",
    "    data_target_iter = iter(x_tr2)\n",
    "    \n",
    "    # Iterate dataloader\n",
    "    for i in tqdm(range(len_dataloader)):\n",
    "        model.train()\n",
    "        # Calculate Alpha\n",
    "        p = float(i + epoch * len_dataloader) / epochs / len_dataloader\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "        \n",
    "        # Reset gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Run model\n",
    "        cl_a1, d_a1, t1, cl1, dl1 = model_metrics(data_source_iter, alpha)\n",
    "        cl_a2, d_a2, t2, cl2, dl2 = model_metrics(data_target_iter, alpha)\n",
    "\n",
    "        # Metrics\n",
    "        class_acc1 += cl_a1.cpu().numpy()\n",
    "        class_acc2 += cl_a2.cpu().numpy()\n",
    "        domain_acc1 += d_a1.cpu().numpy()\n",
    "        domain_acc2 += d_a2.cpu().numpy()\n",
    "        tot1 += t1\n",
    "        tot2 += t2\n",
    "        loss = cl1 + dl1 + dl2 + cl2\n",
    "        # Metrics\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()             # Backpropagation\n",
    "        optimizer.step()            # Update parameters\n",
    "        # lr_sched.step(train_loss)\n",
    "\n",
    "    # Print results\n",
    "    d_acc = (domain_acc1 + domain_acc2)/(tot1 + tot2)\n",
    "    cl1_acc = class_acc1/ tot1\n",
    "    cl2_acc = class_acc2/tot2\n",
    "    loss = train_loss/len_dataloader\n",
    "    \n",
    "    tqdm.write(\n",
    "        f'Domain_Acc: {d_acc:.3f}\\\n",
    "        Class1_Acc: {cl1_acc:.3f}\\\n",
    "        Class2_Acc: {cl2_acc:.3f}\\\n",
    "        Loss: {loss:.3f}',\n",
    "    )\n",
    "    # ____________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ____________________________________________________________________________________________________________\n",
    "# ____________________________________________________________________________________________________________\n",
    "# Helper function to return test metrics\n",
    "def test_model():\n",
    "    # Instanciate metric's variables\n",
    "    train_loss, total = 0, 0\n",
    "    class_acc1, class_acc2, tot1, tot2 = 0, 0, 0, 0\n",
    "    domain_acc1, domain_acc2= 0, 0\n",
    "\n",
    "    # Test parameters\n",
    "    len_dataloader = min(len(x_dev1), len(x_dev2))\n",
    "    data_source_iter = iter(x_dev1)\n",
    "    data_target_iter = iter(x_dev2)\n",
    "    \n",
    "    # Iterate dataloader\n",
    "    for i in tqdm(range(len_dataloader)):\n",
    "        model.eval()\n",
    "        # Calculate Alpha\n",
    "        p = float(i + epoch * len_dataloader) / epochs / len_dataloader\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "        \n",
    "        # Reset gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Run model\n",
    "        cl_a1, d_a1, t1, cl1, dl1 = model_metrics(data_source_iter, alpha)\n",
    "        cl_a2, d_a2, t2, cl2, dl2 = model_metrics(data_target_iter, alpha)\n",
    "\n",
    "        # Metrics\n",
    "        class_acc1 += cl_a1.cpu().numpy()\n",
    "        class_acc2 += cl_a2.cpu().numpy()\n",
    "        domain_acc1 += d_a1.cpu().numpy()\n",
    "        domain_acc2 += d_a2.cpu().numpy()\n",
    "        tot1 += t1\n",
    "        tot2 += t2\n",
    "        loss = cl1 + dl1 + cl2 + dl2\n",
    "    \n",
    "        # Metrics\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Print results\n",
    "    d_acc = (domain_acc1 + domain_acc2)/(tot1 + tot2)\n",
    "    cl1_acc = class_acc1/ tot1\n",
    "    cl2_acc = class_acc2/tot2\n",
    "    loss = train_loss/len_dataloader\n",
    "    \n",
    "    tqdm.write(\n",
    "        f'Domain_Acc: {d_acc:.3f}\\\n",
    "        Class1_Acc: {cl1_acc:.3f}\\\n",
    "        Class2_Acc: {cl2_acc:.3f}\\\n",
    "        Loss: {loss:.3f}',\n",
    "    )\n",
    "    # ____________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BiLSTM network model!\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/118 [00:01<02:17,  1.18s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1014.00 MiB. GPU 0 has a total capacity of 7.79 GiB of which 574.69 MiB is free. Process 1156307 has 2.77 GiB memory in use. Process 1386554 has 4.43 GiB memory in use. Of the allocated memory 3.25 GiB is allocated by PyTorch, and 1020.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m (epoch))\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     test_model()\n",
      "Cell \u001b[0;32mIn[25], line 50\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Run model\u001b[39;00m\n\u001b[1;32m     49\u001b[0m cl_a1, d_a1, t1, cl1, dl1 \u001b[38;5;241m=\u001b[39m model_metrics(data_source_iter, alpha)\n\u001b[0;32m---> 50\u001b[0m cl_a2, d_a2, t2, cl2, dl2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_target_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Metrics\u001b[39;00m\n\u001b[1;32m     53\u001b[0m class_acc1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cl_a1\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m, in \u001b[0;36mmodel_metrics\u001b[0;34m(dataloader_iter, alpha)\u001b[0m\n\u001b[1;32m      5\u001b[0m X, y, text_len, domain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m class_output, domain_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Loss fn\u001b[39;00m\n\u001b[1;32m     11\u001b[0m loss_fn_cl \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss(weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(weights_class(y, \u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 43\u001b[0m, in \u001b[0;36mDANN.forward\u001b[0;34m(self, text, text_lengths, alpha)\u001b[0m\n\u001b[1;32m     41\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(text)\n\u001b[1;32m     42\u001b[0m packed_embedded \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpack_padded_sequence(embedded, text_lengths, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, enforce_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 43\u001b[0m output, (hidden, cell_state) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_embedded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, :, :], hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :, :]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     45\u001b[0m reverse_feature \u001b[38;5;241m=\u001b[39m ReverseLayerF\u001b[38;5;241m.\u001b[39mapply(hidden, alpha)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:881\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    878\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    879\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 881\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    884\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1014.00 MiB. GPU 0 has a total capacity of 7.79 GiB of which 574.69 MiB is free. Process 1156307 has 2.77 GiB memory in use. Process 1386554 has 4.43 GiB memory in use. Of the allocated memory 3.25 GiB is allocated by PyTorch, and 1020.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm, tqdm_notebook # show progress bar\n",
    "\n",
    "# Epochs\n",
    "epochs = 25\n",
    "train_loss, valid_loss1, valid_loss2 = [], [], []\n",
    "print(\"Training BiLSTM network model!\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: %d'% (epoch))\n",
    "    train_model()\n",
    "    test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(10,5))\n",
    "epoch_ticks = range(1, epochs + 1)\n",
    "plt.plot(epoch_ticks, train_loss)\n",
    "plt.plot(epoch_ticks, valid_loss)\n",
    "plt.legend(['Train Loss', 'Valid Loss'])\n",
    "plt.title('Losses') \n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Loss')\n",
    "plt.xticks(epoch_ticks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dl, ln):\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        preds, acc, test_acc = [], 0, 0\n",
    "        for X, y, text_len, domain in dl:\n",
    "            # Dev data\n",
    "            pred, domain_output = model(X, text_len, alpha)\n",
    "            test_acc += torch.sum((pred>=0.5).float() == y)\n",
    "    return (test_acc/ln).detach().cpu().numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(x_tr_dl, df_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(x_dev_dl, df_dev.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('../Data/test_data.json', lines=True)['text']\n",
    "# train = [re.sub(',', '',', '.join([str(x) for x in tok])) for tok in df_train['text']]\n",
    "# test = [[t if t != 0 else 1 for t in ls] for ls in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmasked data\n",
    "import json\n",
    "with open('../Data/test_Data_unmasked.json', 'r') as f:\n",
    "    test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for line in test:\n",
    "    text_tensor = torch.tensor(line).unsqueeze(0).to(device)\n",
    "    text_length = torch.tensor([len(line)])\n",
    "    # Pass the sequence and its length to the model\n",
    "    pred, domain_output = model(text_tensor, text_length, 1)\n",
    "    preds.extend((pred>=0.5).int().detach().cpu().numpy().reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(columns = ['id', 'value'])\n",
    "for idx, v in enumerate(preds):\n",
    "    test_df.loc[idx] = [idx, preds[idx]]\n",
    "test_df.to_csv('../Data/predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Export model\n",
    "model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "model_scripted.save('model_scripted.pt') # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = torch.jit.load('model_scripted.pt')\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
