{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install matplotlib scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    0.92\n",
      "1    0.08\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "df1 = pd.read_json('../Data/domain1_train_data.json', lines=True)\n",
    "df2 = pd.read_json('../Data/domain2_train_data.json', lines=True)\n",
    "\n",
    "# Define Domains\n",
    "df1['domain'], df2['domain'] = 0, 1\n",
    "\n",
    "# Split set 1\n",
    "df1_train, df1_dev = train_test_split(df1, stratify=df1['label'], random_state=0, test_size=0.2)\n",
    "# Split set 2\n",
    "x2_1 = df2[df2['label'] == 1].sample(500, random_state=0)\n",
    "x2_0 = df2[df2['label'] == 0].sample(500, random_state=0)\n",
    "df2_train = df2[[i not in list(pd.concat([x2_1, x2_0]).reset_index()['index']) for i in df2.index]].reset_index(drop=True)\n",
    "df2_dev = pd.concat([x2_1,x2_0]).reset_index(drop=True)\n",
    "\n",
    "# Print classes proportion\n",
    "print(round(df2_train['label'].value_counts()/len(df2_train['label']),2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 DL Models (BoW + BiLSTM + DANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4b174c43d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare pytorch dataset\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, text, labels, domain):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.domain = domain\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.tensor(self.text[idx])\n",
    "        label = torch.tensor(self.labels[idx]).reshape(-1,1)\n",
    "        domain = torch.tensor(self.domain[idx])\n",
    "        return text, label, domain\n",
    "    \n",
    "# Define collate (pre_process) function\n",
    "def collate_batch(batch):  \n",
    "    texts, labels, domain = zip(*batch)\n",
    "    text_len = [len(txt) for txt in texts]\n",
    "    text = nn.utils.rnn.pad_sequence(texts, batch_first=True).to(device)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32).to(device).reshape(-1,1)\n",
    "    domain = torch.tensor(domain, dtype=torch.float32).to(device).reshape(-1,1)\n",
    "    return text, labels, text_len, domain\n",
    "\n",
    "# Reset indexes\n",
    "df1_train.reset_index(drop=True, inplace=True)\n",
    "df2_train.reset_index(drop=True, inplace=True)\n",
    "df1_dev.reset_index(drop=True, inplace=True)\n",
    "df2_dev.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create datasets\n",
    "train_DS1 = Dataset(df1_train['text'], df1_train['label'], df1_train['domain'])\n",
    "train_DS2 = Dataset(df2_train['text'], df2_train['label'], df2_train['domain'])\n",
    "dev_DS1 = Dataset(df1_dev['text'], df1_dev['label'], df1_dev['domain'])\n",
    "dev_DS2 = Dataset(df2_dev['text'], df2_dev['label'], df2_dev['domain'])\n",
    "\n",
    "# Sample train data\n",
    "# sampler_tr1 = torch.utils.data.WeightedRandomSampler(weights(df1_train), num_samples=len(train_DS1), replacement=True)\n",
    "sampler_tr2 = torch.utils.data.WeightedRandomSampler(weights(df2_train), num_samples=len(train_DS2), replacement=True)\n",
    "\n",
    "# Create dataloaders\n",
    "bs = 32\n",
    "x_tr1 = DataLoader(train_DS1, batch_size=bs, collate_fn=collate_batch)\n",
    "x_tr2 = DataLoader(train_DS2, batch_size=bs, collate_fn=collate_batch, sampler=sampler_tr2)\n",
    "x_dev1 = DataLoader(dev_DS1, batch_size=bs, collate_fn=collate_batch)\n",
    "x_dev2 = DataLoader(dev_DS2, batch_size=bs, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "class ReverseLayerF(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "        return output, None\n",
    "\n",
    "def reverse_gradient(x, alpha=7):\n",
    "    return ReverseLayerF.apply(x, alpha)\n",
    "\n",
    "# Bidirectional LSTM model\n",
    "class DANN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers):\n",
    "        super(DANN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Feature extraction Layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, num_layers=n_layers, batch_first=True,dropout = 0.5)\n",
    "\n",
    "        # Classiffier layer\n",
    "        self.class_classifier = nn.Sequential()\n",
    "        self.class_classifier.add_module('fc1', nn.Linear(hidden_dim*2, 1))\n",
    "        self.class_classifier.add_module('dropout', nn.Dropout(0.2))\n",
    "        self.class_classifier.add_module('Sigmoid', nn.Sigmoid())\n",
    "\n",
    "        # Domain classifier Layer\n",
    "        self.domain_classifier = nn.Sequential()\n",
    "        self.domain_classifier.add_module('fc1', nn.Linear(hidden_dim*2, hidden_dim))\n",
    "        self.domain_classifier.add_module('relu', nn.ReLU())\n",
    "        self.domain_classifier.add_module('fc2', nn.Linear(hidden_dim, 1))\n",
    "        self.domain_classifier.add_module('sigmoid', nn.Sigmoid())\n",
    "\n",
    "    def forward(self, text, text_lengths, alpha):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
    "        output, (hidden, cell_state) = self.lstm(packed_embedded)\n",
    "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        reverse_feature = ReverseLayerF.apply(hidden, alpha)\n",
    "        class_output = self.class_classifier(hidden)\n",
    "        domain_output = self.domain_classifier(reverse_feature)\n",
    "        \n",
    "        return class_output, domain_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANN(\n",
      "  (embedding): Embedding(90000, 128, padding_idx=0)\n",
      "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (class_classifier): Sequential(\n",
      "    (fc1): Linear(in_features=512, out_features=1, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (Sigmoid): Sigmoid()\n",
      "  )\n",
      "  (domain_classifier): Sequential(\n",
      "    (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "h_dim, e_dim = 256, 128\n",
    "model = DANN(vocab_size=90000, embedding_dim=e_dim, hidden_dim=h_dim, n_layers=2).to(device)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.005, patience=5,)\n",
    "\n",
    "# Weights function\n",
    "def weights_class(y, c):\n",
    "    y = pd.Series(y.int().numpy(force=True).reshape(-1))\n",
    "    w = len(y)/y.value_counts()\n",
    "    if c == 0:\n",
    "        try:\n",
    "            return w[0]\n",
    "        except:\n",
    "            return 1\n",
    "    else:\n",
    "        try:\n",
    "            return w[1]\n",
    "        except:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ____________________________________________________________________________________________________________\n",
    "# Function to get models' metrics\n",
    "def model_metrics(dataloader_iter, alpha):\n",
    "    # training model using source data\n",
    "    X, y, text_len, domain = next(dataloader_iter)\n",
    "    # Predict\n",
    "    class_output, domain_output = model(X, text_len, alpha)\n",
    "    # Loss fn\n",
    "    loss_fn_cl = nn.BCELoss(weight = torch.tensor(weights_class(y, 0)).to(device))\n",
    "    loss_fn_d = nn.BCELoss(weight = torch.tensor(weights_class(domain, 1)).to(device))\n",
    "    # Classifier metrics\n",
    "    class_loss = loss_fn_cl(class_output, y) \n",
    "    class_acc = torch.sum((class_output>=0.5).float() == y)\n",
    "    # Discriminator metrics\n",
    "    domain_loss = loss_fn_d(domain_output, domain)\n",
    "    domain_acc = torch.sum((domain_output>=0.5).float() == domain)\n",
    "    total = y.size()[0]\n",
    "    return class_acc, domain_acc, total, class_loss, domain_loss\n",
    "# ____________________________________________________________________________________________________________\n",
    "# ____________________________________________________________________________________________________________\n",
    "# Helper function to return training metrics\n",
    "def train_model():\n",
    "    # Instanciate metric's variables\n",
    "    train_loss, total = 0, 0\n",
    "    class_acc1, class_acc2, tot1, tot2 = 0, 0, 0, 0\n",
    "    domain_acc1, domain_acc2= 0, 0\n",
    "    # Train parameters\n",
    "    len_dataloader = min(len(x_tr1), len(x_tr2))\n",
    "    data_source_iter = iter(x_tr1)\n",
    "    data_target_iter = iter(x_tr2)\n",
    "    # Iterate dataloader\n",
    "    for i in tqdm(range(len_dataloader)):\n",
    "        model.train()\n",
    "        # Calculate Alpha\n",
    "        p = float(i + epoch * len_dataloader) / epochs / len_dataloader\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "        \n",
    "        # Reset gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Run model\n",
    "        cl_a1, d_a1, t1, cl1, dl1 = model_metrics(data_source_iter, alpha)\n",
    "        cl_a2, d_a2, t2, cl2, dl2 = model_metrics(data_target_iter, alpha)\n",
    "        # Metrics\n",
    "        class_acc1 += cl_a1.cpu().numpy()\n",
    "        class_acc2 += cl_a2.cpu().numpy()\n",
    "        domain_acc1 += d_a1.cpu().numpy()\n",
    "        domain_acc2 += d_a2.cpu().numpy()\n",
    "        tot1 += t1\n",
    "        tot2 += t2\n",
    "        loss = cl1 + cl2\n",
    "        # Metrics\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()             # Backpropagation\n",
    "        optimizer.step()            # Update parameters\n",
    "        # lr_sched.step(train_loss)\n",
    "    # Print results\n",
    "    d_acc = (domain_acc1 + domain_acc2)/(tot1 + tot2)\n",
    "    cl1_acc = class_acc1/ tot1\n",
    "    cl2_acc = class_acc2/tot2\n",
    "    loss = train_loss/len_dataloader\n",
    "    \n",
    "    tqdm.write(\n",
    "        f'Domain_Acc: {d_acc:.3f}\\\n",
    "        Class1_Acc: {cl1_acc:.3f}\\\n",
    "        Class2_Acc: {cl2_acc:.3f}\\\n",
    "        Loss: {loss:.3f}',\n",
    "    )\n",
    "    # ____________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ____________________________________________________________________________________________________________\n",
    "# ____________________________________________________________________________________________________________\n",
    "# Helper function to return test metrics\n",
    "def test_model():\n",
    "    # Instanciate metric's variables\n",
    "    test_loss, total = 0, 0\n",
    "    class_acc1, class_acc2, tot1, tot2 = 0, 0, 0, 0\n",
    "    domain_acc1, domain_acc2= 0, 0\n",
    "\n",
    "    # Test parameters\n",
    "    len_dataloader = min(len(x_dev1), len(x_dev2))\n",
    "    data_source_iter = iter(x_dev1)\n",
    "    data_target_iter = iter(x_dev2)\n",
    "    \n",
    "    # Iterate dataloader\n",
    "    for i in tqdm(range(len_dataloader)):\n",
    "        model.eval()\n",
    "        # Calculate Alpha\n",
    "        p = float(i + epoch * len_dataloader) / epochs / len_dataloader\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "        \n",
    "        # Reset gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Run model\n",
    "        cl_a1, d_a1, t1, cl1, dl1 = model_metrics(data_source_iter, alpha)\n",
    "        cl_a2, d_a2, t2, cl2, dl2 = model_metrics(data_target_iter, alpha)\n",
    "\n",
    "        # Metrics\n",
    "        class_acc1 += cl_a1.cpu().numpy()\n",
    "        class_acc2 += cl_a2.cpu().numpy()\n",
    "        domain_acc1 += d_a1.cpu().numpy()\n",
    "        domain_acc2 += d_a2.cpu().numpy()\n",
    "        tot1 += t1\n",
    "        tot2 += t2\n",
    "        loss = cl1 + dl2\n",
    "    \n",
    "        # Metrics\n",
    "        test_loss += loss.item()\n",
    "\n",
    "    # Print results\n",
    "    d_acc = (domain_acc1 + domain_acc2)/(tot1 + tot2)\n",
    "    cl1_acc = class_acc1/ tot1\n",
    "    cl2_acc = class_acc2/tot2\n",
    "    loss = test_loss/len_dataloader\n",
    "    \n",
    "    tqdm.write(\n",
    "        f'Domain_Acc: {d_acc:.3f}\\\n",
    "        Class1_Acc: {cl1_acc:.3f}\\\n",
    "        Class2_Acc: {cl2_acc:.3f}\\\n",
    "        Loss: {loss:.3f}',\n",
    "    )\n",
    "    # ____________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BiLSTM network model!\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:24<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.500        Class1_Acc: 0.574        Class2_Acc: 0.601        Loss: 2.745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 13.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.500        Class1_Acc: 0.622        Class2_Acc: 0.648        Loss: 2.008\n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:23<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.502        Class1_Acc: 0.644        Class2_Acc: 0.689        Loss: 2.480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 13.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.502        Class1_Acc: 0.656        Class2_Acc: 0.720        Loss: 1.962\n",
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:23<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.502        Class1_Acc: 0.684        Class2_Acc: 0.714        Loss: 2.246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 12.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.521        Class1_Acc: 0.678        Class2_Acc: 0.681        Loss: 1.910\n",
      "epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:23<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.502        Class1_Acc: 0.773        Class2_Acc: 0.791        Loss: 1.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 12.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.494        Class1_Acc: 0.699        Class2_Acc: 0.698        Loss: 1.969\n",
      "epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:23<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.515        Class1_Acc: 0.811        Class2_Acc: 0.832        Loss: 1.369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 12.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.490        Class1_Acc: 0.734        Class2_Acc: 0.742        Loss: 1.881\n",
      "epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:23<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.510        Class1_Acc: 0.849        Class2_Acc: 0.861        Loss: 1.086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 12.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.479        Class1_Acc: 0.747        Class2_Acc: 0.726        Loss: 2.065\n",
      "epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:23<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.534        Class1_Acc: 0.868        Class2_Acc: 0.875        Loss: 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 12.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.524        Class1_Acc: 0.754        Class2_Acc: 0.725        Loss: 2.152\n",
      "epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:23<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.517        Class1_Acc: 0.882        Class2_Acc: 0.882        Loss: 0.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 13.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.477        Class1_Acc: 0.753        Class2_Acc: 0.709        Loss: 2.384\n",
      "epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:23<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.496        Class1_Acc: 0.888        Class2_Acc: 0.886        Loss: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.482        Class1_Acc: 0.747        Class2_Acc: 0.702        Loss: 2.547\n",
      "epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:23<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.482        Class1_Acc: 0.890        Class2_Acc: 0.884        Loss: 0.743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 13.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.472        Class1_Acc: 0.738        Class2_Acc: 0.713        Loss: 2.645\n",
      "epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:23<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.489        Class1_Acc: 0.886        Class2_Acc: 0.888        Loss: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 12.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.476        Class1_Acc: 0.738        Class2_Acc: 0.678        Loss: 2.874\n",
      "epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 22/125 [00:04<00:21,  4.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m (epoch))\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     test_model()\n",
      "Cell \u001b[0;32mIn[10], line 53\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Metrics\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 53\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m             \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()            \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# lr_sched.step(train_loss)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm, tqdm_notebook # show progress bar\n",
    "\n",
    "# Epochs\n",
    "epochs = 25\n",
    "train_loss, valid_loss1, valid_loss2 = [], [], []\n",
    "print(\"Training BiLSTM network model!\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: %d'% (epoch))\n",
    "    train_model()\n",
    "    test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dl, ln):\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        preds, acc, test_acc = [], 0, 0\n",
    "        for X, y, text_len, domain in dl:\n",
    "            # Dev data\n",
    "            pred, domain_output = model(X, text_len, alpha)\n",
    "            test_acc += torch.sum((pred>=0.5).float() == y)\n",
    "    return (test_acc/ln).detach().cpu().numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(x_tr_dl, df_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(x_dev_dl, df_dev.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('../Data/test_data.json', lines=True)['text']\n",
    "# train = [re.sub(',', '',', '.join([str(x) for x in tok])) for tok in df_train['text']]\n",
    "# test = [[t if t != 0 else 1 for t in ls] for ls in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmasked data\n",
    "import json\n",
    "with open('../Data/test_Data_unmasked.json', 'r') as f:\n",
    "    test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for line in test:\n",
    "    text_tensor = torch.tensor(line).unsqueeze(0).to(device)\n",
    "    text_length = torch.tensor([len(line)])\n",
    "    # Pass the sequence and its length to the model\n",
    "    pred, domain_output = model(text_tensor, text_length, 1)\n",
    "    preds.extend((pred>=0.5).int().detach().cpu().numpy().reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(columns = ['id', 'value'])\n",
    "for idx, v in enumerate(preds):\n",
    "    test_df.loc[idx] = [idx, preds[idx]]\n",
    "test_df.to_csv('../Data/predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Export model\n",
    "model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "model_scripted.save('model_scripted.pt') # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = torch.jit.load('model_scripted.pt')\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
